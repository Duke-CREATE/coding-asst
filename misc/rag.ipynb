{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import json\n",
    "\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('secret.json', 'r') as json_file:\n",
    "    key = json.load(json_file)['key']\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str, db):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = db.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function to take in chat history, the query an return a response\n",
    "\n",
    "def get_response(db, chat, query, chat_history = []):\n",
    "    prompt = HumanMessage(\n",
    "    content=augment_prompt(query, db=db),\n",
    "    )\n",
    "\n",
    "    # get the response from the model\n",
    "    chat_history.append(prompt)\n",
    "\n",
    "    response = chat(chat_history)\n",
    "\n",
    "    return response.content, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs - PDF Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_docs = \"documents\"\n",
    "\n",
    "subject = \"aipi530\"\n",
    "\n",
    "sub = \"material\"\n",
    "\n",
    "doc_path = f\"{base_docs}/{subject}/{sub}/\"\n",
    "\n",
    "loader = PyPDFDirectoryLoader(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings(model=\"text-embedding-ada-002\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Backpack packing\n",
      "2. Production Optimization\n",
      "3. Network Optimization\n",
      "4. Media Advertisement\n",
      "5. Hospital Management\n",
      "6. Portfolio Management\n",
      "7. Price Optimization\n",
      "8. Personnel Assignment\n",
      "9. Transportation Scheduling\n",
      "10. Inventory Management\n",
      "11. Facility Location\n"
     ]
    }
   ],
   "source": [
    "query = \"Enumrate the different use cases\"\n",
    "\n",
    "response, messages = get_response(db=db, chat=chat, query=query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first use case is \"Backpack packing\". It uses the Mixed Integer Linear Programming (MILP) method. The type of problem it addresses is the \"Knapsack\" problem. It does not involve Monte Carlo simulations (MC).\n"
     ]
    }
   ],
   "source": [
    "query = \"Describe the first use case in detail\"\n",
    "\n",
    "response, messages = get_response(db=db, chat=chat, query=query, chat_history=messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs - Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "from langchain.document_loaders.directory import DirectoryLoader\n",
    "from langchain.document_loaders import PythonLoader\n",
    "\n",
    "loader = DirectoryLoader('python_files', glob=\"**/*.py\", loader_cls=PythonLoader)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings(model=\"text-embedding-ada-002\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code in the file primarily performs the following tasks:\n",
      "\n",
      "1. Clones a repository (LangChain, in this case) from GitHub to a local path.\n",
      "2. Loads the python files from the cloned repository using a GenericLoader.\n",
      "3. Splits the loaded documents into chunks using a RecursiveCharacterTextSplitter.\n",
      "4. Adds the split texts to a Chroma vectorstore and creates a retriever.\n",
      "5. Defines a prompt template for a Retrieval Augmented Generation (RAG) model. \n",
      "6. Defines a concise summary prompt template. \n",
      "\n",
      "The code appears to be part of a larger system for processing and analyzing codebases, possibly for the purpose of question answering or text generation using advanced language models.\n"
     ]
    }
   ],
   "source": [
    "query = \"Summarize the code in the file\"\n",
    "\n",
    "response, messages = get_response(db=db, chat=chat, query=query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "create_center",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
